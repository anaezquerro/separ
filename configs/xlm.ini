[enc_conf]

[word_conf]
pretrained = xlm-roberta-large
finetune = True 

[train_conf]
batch_size = 300
lr = 1e-5
epochs = 100
train_patience = 5
dev_patience = 10
max_norm = 5.0
steps = 3