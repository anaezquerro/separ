[enc_conf]
hidden_size = 400

[word_conf]
pretrained = xlm-roberta-large
finetune = True 

[train_conf]
batch_size = 100
lr = 1e-5
epochs = 50
train_patience = 20
dev_patience = 20

[dec_conf]
tsched = lin
num_layers = 6
num_heads = 16
tol = 0.03
patience = 3
loss = 2